{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import random\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from sympy import simplify\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "from async_engine.batched_api import BatchingAPI\n",
    "from async_engine.api import API\n",
    "\n",
    "from src.prompts.adapt import gameof24 as llama_prompts\n",
    "from utils import parse_suggestions, create_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State class\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GameOf24State:\n",
    "    # game of 24 puzzle, for example 1 1 4 6\n",
    "    puzzle: str\n",
    "\n",
    "    # initialized to the same value as puzzle, but is updated as the game progresses\n",
    "    current_state: str\n",
    "\n",
    "    steps: List[str]\n",
    "\n",
    "    #Randomness used for resampling (random seed)\n",
    "    randomness: int\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.puzzle, self.current_state, \" -> \".join(self.steps)))\n",
    "    \n",
    "    def items(self):\n",
    "        return self.puzzle, self.current_state, self.steps, self.randomness\n",
    "    \n",
    "    def duplicate(self, randomness=None):\n",
    "        return GameOf24State(\n",
    "            puzzle=self.puzzle,\n",
    "            current_state=self.current_state,\n",
    "            steps=self.steps,\n",
    "            randomness=randomness if randomness is not None else self.randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reflexion agent :O\n",
    "\n",
    "class GameOf24Agent:\n",
    "\n",
    "    @staticmethod\n",
    "    async def step(state: GameOf24State, api, namespace, reflexion: list)-> GameOf24State:\n",
    "        \"\"\"\n",
    "        Given a state, returns the next state one.\n",
    "        \"\"\"\n",
    "\n",
    "        # set up the prompt, based on the current state\n",
    "\n",
    "        # ToT uses bfs_prompt to generate next steps but then uses\n",
    "        # the cot_prompt to get the final expression. \n",
    "        # For example, input : 1 1 4 6\n",
    "        # Step 0 : '1 - 1 = 0 (left: 0 4 6)'          BFS prompt\n",
    "        # Step 1 : '0 + 4 = 4 (left: 4 6)'            BFS prompt\n",
    "        # Step 2 : '4 * 6 = 24 (left: 24)'            BFS prompt\n",
    "        # Step 3 : Answer : ((1 - 1) + 4) * 6 = 24    CoT prompt\n",
    "\n",
    "\n",
    "        # set up the prompt, based on the current state\n",
    "        current_state = state.current_state\n",
    "        \n",
    "        if current_state.strip() == \"24\":\n",
    "            # CoT prompt\n",
    "            steps = \"\\n\".join(state.steps) + \"\\n\"\n",
    "            \n",
    "            prompt = llama_prompts.cot_prompt.format(input=state.puzzle) + \"Steps:\\n\" + steps + \"Answer: \"\n",
    "\n",
    "            # Get the final expression\n",
    "            suggestions = await api.buffered_request(prompt, key=hash(state), namespace=namespace)\n",
    "\n",
    "            # State does not change, only the steps\n",
    "            selected_suggestion = suggestions\n",
    "            selected_state = state.current_state\n",
    "            \n",
    "\n",
    "\n",
    "        else:\n",
    "            if len(reflexion) == 0:\n",
    "                prompt = llama_prompts.bfs_prompt.format(input=current_state) \n",
    "            else:\n",
    "                prompt = llama_prompts.bfs_reflexion_prompt.format(input=current_state, puzzle = \"1 1 4 6\", reflexion=reflexion[0]) \n",
    "\n",
    "            suggestions = await api.buffered_request(prompt, key=hash(state), namespace=namespace)\n",
    "\n",
    "            # parse suggestions, based on the current state\n",
    "            parsed_suggestions = parse_suggestions(suggestions)\n",
    "            if parsed_suggestions == []:\n",
    "                print(f\"No suggestions were parsed from state: {state}\")\n",
    "                print(f\"\\nPrompt: {prompt}\\nSuggestions: {suggestions}\\nParsed suggestions: {' | '.join(parsed_suggestions)}\\n\")\n",
    "                assert False, \"No suggestions found.\"\n",
    "            \n",
    "            suggestions = parsed_suggestions\n",
    "            \n",
    "            random.seed(state.randomness)\n",
    "            selected_suggestion = random.choice(suggestions)\n",
    "            selected_state = GameOf24Agent.parse_next_state(selected_suggestion)\n",
    "\n",
    "        # set up new state object\n",
    "        next_state = GameOf24State(\n",
    "            puzzle=state.puzzle,\n",
    "            current_state=selected_state,\n",
    "            steps=state.steps + [selected_suggestion],\n",
    "            randomness=random.randint(0, 1000)\n",
    "        )\n",
    "        return next_state\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_next_state(suggestion: str) -> str:\n",
    "        return suggestion.split('left: ')[-1].split(')')[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def verify(state: GameOf24State)-> dict:\n",
    "            \"\"\"\n",
    "            Verifies the output of a given task\n",
    "                1. Checks if the numbers used are the same as the ones provided.\n",
    "                2. Checks if the operations performed result to 24.\n",
    "\n",
    "            States \n",
    "                {\"r\": 0} : Not finished.\n",
    "                {\"r\": 1} : Finished and correct.\n",
    "                {\"r\": -1} : Finished and incorrect.\n",
    "            \"\"\"\n",
    "            current_states = state.current_state.split(\" \")\n",
    "            if len(current_states) !=1 or len(state.steps)<=3:\n",
    "                # More than one number left\n",
    "                return {'r':0}\n",
    "            elif current_states[0] != \"24\":\n",
    "                # One number left and it is not 24\n",
    "                return {'r':-1}\n",
    "            else:\n",
    "                # One number left and it is 24\n",
    "                expression = state.steps[-1].lower().replace('answer: ', '').split('=')[0]\n",
    "                numbers = re.findall(r'\\d+', expression)\n",
    "                problem_numbers = re.findall(r'\\d+', state.puzzle)\n",
    "                if sorted(numbers) != sorted(problem_numbers):\n",
    "                    # Numbers used are not the same as the ones provided\n",
    "                    return {'r': -1}\n",
    "                try:\n",
    "                    if simplify(expression) == 24:\n",
    "                        return {'r': 1}\n",
    "                    else:\n",
    "                        # Operations performed do not result to 24\n",
    "                        return {'r': -1}\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    return {'r': -1}\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_reflexion(puzzle: str, steps, state: GameOf24State, api, namespace) -> str:\n",
    "        prompt = llama_prompts.reflexion_prompt.format(puzzle=puzzle, steps=steps)\n",
    "        reflexion = api.buffered_request(prompt, key=hash(state), namespace=namespace)\n",
    "        return reflexion\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_summary(reflexion, state: GameOf24State, api, namespace) -> str:\n",
    "        prompt = llama_prompts.summary_prompt.format(reflexion=reflexion)\n",
    "        reflexion = api.buffered_request(prompt, key=hash(state), namespace=namespace)\n",
    "        return reflexion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve 1 1 4 6 puzzle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazykey import AsyncKeyHandler\n",
    "from groq import AsyncGroq\n",
    "\n",
    "step_api_config = eval_api_config = {\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"request_timeout\": 120,\n",
    "    \"top_k\": 50\n",
    "}\n",
    "\n",
    "model = \"llama-3.3-70b-versatile\"\n",
    "provider = \"LazyKey\"\n",
    "models = {\n",
    "    \"step\": {\"model_name\":model, \"provider\":provider},\n",
    "    \"eval\": {\"model_name\":model, \"provider\":provider},\n",
    "}\n",
    "\n",
    "api = API(eval_api_config, models=models.values(), resources=2, verbose=False)\n",
    "\n",
    "puzzle = \"1 1 4 6\"\n",
    "num_agents = 2\n",
    "\n",
    "\n",
    "step_batcher = BatchingAPI(api, batch_size=1, timeout=2, model=models[\"step\"][\"model_name\"], tab=\"step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to solve the puzzle\n",
    "async def solvePuzzle(num_steps, agent_reflexions):\n",
    "    global num_agents\n",
    "    #Create initial state/environment\n",
    "    states =  {}\n",
    "    for agent_id in range(num_agents):\n",
    "        states[agent_id] = GameOf24State(puzzle=puzzle, current_state=puzzle, steps=[], randomness=random.randint(0,1000))\n",
    "        \n",
    "    finished_states = {}\n",
    "    #Stepping\n",
    "    for step in range(num_steps):\n",
    "        print(f\"Step {step} : Stepping\")\n",
    "        agent_tasks = [\n",
    "            asyncio.create_task(\n",
    "            GameOf24Agent.step(states[agent_id], step_batcher, namespace=(0, f\"Agent: {agent_id}\", f\"Step : {step}\"), reflexion=agent_reflexions[agent_id])\n",
    "            )\n",
    "            for agent_id in states\n",
    "        ]\n",
    "        new_states = await asyncio.gather(*agent_tasks)\n",
    "\n",
    "        for agent_id, new_state in zip(states.keys(), new_states):\n",
    "            states[agent_id] = new_state\n",
    "            print(f\"Current step for agent {agent_id}: {new_state.steps[-1]} \\n\")\n",
    "\n",
    "        # Evaluate whether a puzzle has been solved\n",
    "        for agent_id in list(states.keys()):\n",
    "            if GameOf24Agent.verify(states[agent_id]) == {\"r\": 1}:\n",
    "                print(f\"Puzzle finished by agent {agent_id}: {states[agent_id].puzzle}\")\n",
    "                finished_states[agent_id] = states.pop(agent_id)\n",
    "                num_agents -=1\n",
    "\n",
    "        # If all puzzles have been solved, break\n",
    "        if not states:\n",
    "            break\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def makeReflexion(reflexion_type, num_reflexions, k, states, agent_reflexions, agent_all_reflexions, summary_method):\n",
    "    step = 3\n",
    "    for agent_id in states:\n",
    "        print(\"agent_id: \", agent_id)\n",
    "    agent_tasks = [\n",
    "        asyncio.create_task(\n",
    "            GameOf24Agent.generate_reflexion(puzzle=puzzle, steps=states[agent_id].steps, state=states[agent_id], api=step_batcher, namespace=(0, f\"Agent: {agent_id}\", f\"Step: {step}\")\n",
    "        )\n",
    "    )\n",
    "    for agent_id in states\n",
    "    ] \n",
    "\n",
    "    new_reflexions = await asyncio.gather(*agent_tasks)\n",
    "    for agent_id, reflexion in zip(states.keys(), new_reflexions):\n",
    "        agent_reflexions[agent_id].append(reflexion)\n",
    "        agent_all_reflexions[agent_id].append(reflexion)\n",
    "\n",
    "    if reflexion_type == \"list\":\n",
    "        return agent_reflexions, agent_all_reflexions\n",
    "\n",
    "    elif reflexion_type == \"k most recent\":\n",
    "        for agent_id in agent_reflexions:\n",
    "            agent_reflexions[agent_id] = agent_reflexions[agent_id][-k:]\n",
    "        return agent_reflexions, agent_all_reflexions\n",
    "\n",
    "    elif reflexion_type == \"summary\":\n",
    "        #Right now makes summary of earlier summary + new reflexions, \n",
    "        # if we want to change this we need to return reflexion and summary, pass summary to solvePuzzle, pass reflexion to makeReflexion\n",
    "        agent_summaries = []\n",
    "        if summary_method == \"incremental\":\n",
    "            agent_summaries = [\n",
    "                asyncio.create_task(\n",
    "                GameOf24Agent.generate_summary(agent_reflexions[agent_id], \n",
    "                state=states[agent_id], api=step_batcher, namespace=(0, f\"Agent: {agent_id}\", f\"Step : {step}\")\n",
    "                    )\n",
    "                )\n",
    "                for agent_id in states\n",
    "            ]\n",
    "        elif summary_method == \"all_previous\":\n",
    "            agent_summaries = [\n",
    "                asyncio.create_task(\n",
    "                GameOf24Agent.generate_summary(reflexion=agent_all_reflexions[agent_id], \n",
    "                state=states[agent_id], api=step_batcher, namespace=(0, f\"Agent: {agent_id}\", f\"Step : {step}\")\n",
    "                    )\n",
    "                )\n",
    "                for agent_id in states\n",
    "            ]\n",
    "        summaries = await asyncio.gather(*agent_summaries)\n",
    "        for agent_id, summary in zip(states.keys(), summaries):\n",
    "            agent_reflexions[agent_id] = [summary] #Replaces reflexions with summary\n",
    "        return agent_reflexions, agent_all_reflexions\n",
    "    else:\n",
    "        print(\"unknown type\")\n",
    "        return agent_reflexions, agent_all_reflexions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runReflexionGameOf24(typeOfReflexion, num_iterations, k, summary_method=\"incremental\"):\n",
    "    agent_reflexions = {}\n",
    "    agent_all_reflexions = {}\n",
    "    num_steps = 4\n",
    "\n",
    "    for agent_id in range(num_agents):\n",
    "        agent_reflexions[agent_id] = []\n",
    "        agent_all_reflexions[agent_id] = []\n",
    "    #Without reflexion first\n",
    "    states = await solvePuzzle(num_steps, agent_reflexions)\n",
    "    #Reflect and go again i times\n",
    "    for i in range(num_iterations):\n",
    "        agent_reflexions, agent_all_reflexions = await makeReflexion(typeOfReflexion, i+1, k, states, agent_reflexions, agent_all_reflexions, summary_method)\n",
    "        print(\"reflexions per agent\", agent_reflexions)\n",
    "        states = await solvePuzzle(num_steps, agent_reflexions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : Stepping\n",
      "Current step for agent 0: 1 - 1 = 0 (left: 0 4 6) \n",
      "\n",
      "Current step for agent 1: 6 + 1 = 7 (left: 1 4 7) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 0 * 6 = 0 (left: 0 4) \n",
      "\n",
      "Current step for agent 1: 1 - 7 = -6 (left: -6 4) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: 4 - 0 = 4 (left: 4) \n",
      "\n",
      "Current step for agent 1: -6 * 4 = -24 (left: -24) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: 4 * 4 = 16 (left: 16) \n",
      "\n",
      "Current step for agent 1: -24 * -24 = 576 (left: 576) \n",
      "\n",
      "agent_id:  0\n",
      "agent_id:  1\n",
      "reflexions per agent {0: ['The previous attempt contains a mistake. The error occurs when the operation \"4 * 4 = 16\" is performed, as it uses the number 4 twice, which is not allowed in the game of 24 where each number can only be used once.\\n\\nThis mistake can be avoided by ensuring that each number is used only once throughout the solution. A general reflection is that players should carefully track the numbers used in each step to prevent reuse and explore alternative operations that utilize all four numbers exactly once.'], 1: [\"The previous attempt contains a mistake. The error occurs when the operation results in a value that is not one of the original numbers, and then using that result to perform another operation. In this case, the mistake is using -24 (result of -6 * 4) and multiplying it by itself, which is not a valid step in the game of 24 as it introduces an extra number.\\n\\nThis mistake can be avoided by ensuring that each operation uses only the original numbers or a result that can be used as an intermediate step to get closer to 24, without introducing extra numbers. A general reflection is to carefully track the numbers used in each step and verify that the operations are valid according to the game's rules.\"]}\n",
      "Step 0 : Stepping\n",
      "Current step for agent 0: 1 * 1 = 1 (left: 1 4 6) \n",
      "\n",
      "Current step for agent 1: 1 / 1 = 1 (left: 1 4 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 1 + 4 = 5 (left: 5 6) \n",
      "\n",
      "Current step for agent 1: 1 + 4 = 5 (left: 5 6) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: 5 - 6 = -1 (left: -1) \n",
      "\n",
      "Current step for agent 1: 6 / 5 = 1.2 (not allowed, result is not an integer, but)  \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: There are no possible next steps with only one number. \n",
      "\n",
      "Current step for agent 1: 6 + 5 = 11 (left: 11) \n",
      "\n",
      "agent_id:  0\n",
      "agent_id:  1\n",
      "reflexions per agent {0: ['The previous attempt contains a mistake. The error occurs when the operation \"4 * 4 = 16\" is performed, as it uses the number 4 twice, which is not allowed in the game of 24 where each number can only be used once.\\n\\nThis mistake can be avoided by ensuring that each number is used only once throughout the solution. A general reflection is that players should carefully track the numbers used in each step to prevent reuse and explore alternative operations that utilize all four numbers exactly once.', 'The previous attempt at solving the puzzle contains a mistake in its approach. The mistake lies in the sequential application of operations without considering the overall goal of reaching 24. Specifically, the step \"5 - 6 = -1\" leads to a dead end, as subtracting a larger number from a smaller one results in a negative number, making it impossible to reach 24 with the remaining operations.\\n\\nThis mistake can be generalized as a lack of foresight in operation selection. To avoid similar mistakes, players should consider the potential outcomes of each operation and choose the ones that keep the result closer to 24 or provide more opportunities for subsequent operations. A more effective approach would involve evaluating the numbers and selecting operations that combine them in a way that creates multiples of the target number or its factors, increasing the chances of reaching 24.'], 1: [\"The previous attempt contains a mistake. The error occurs when the operation results in a value that is not one of the original numbers, and then using that result to perform another operation. In this case, the mistake is using -24 (result of -6 * 4) and multiplying it by itself, which is not a valid step in the game of 24 as it introduces an extra number.\\n\\nThis mistake can be avoided by ensuring that each operation uses only the original numbers or a result that can be used as an intermediate step to get closer to 24, without introducing extra numbers. A general reflection is to carefully track the numbers used in each step and verify that the operations are valid according to the game's rules.\", 'The mistake in the previous attempt is the division operation that resulted in a non-integer value (6 / 5 = 1.2), which is not allowed in the game of 24. This mistake can be avoided by carefully considering the operations and their potential outcomes before proceeding. A general reflection is that players should prioritize operations that are likely to yield integer results, especially when dividing, to increase the chances of finding a valid solution.']}\n",
      "Step 0 : Stepping\n",
      "Current step for agent 0: 4 - 6 = -2 (left: -2 1 1) \n",
      "\n",
      "Current step for agent 1: 1 / 1 = 1 (left: 1 4 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: -2 / 1 = -2 (left: -2 1) \n",
      "\n",
      "Current step for agent 1: 1 * 4 = 4 (left: 4 6) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: -2 / 1 = -2 (left: -2) \n",
      "\n",
      "Current step for agent 1: 4 - 6 = -2 (left: -2) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: There are no possible next steps with only one number. \n",
      "\n",
      "Current step for agent 1: -2 - -2 = 0 (left: 0) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "await runReflexionGameOf24(\"list\", 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : Stepping\n",
      "Current step for agent 0: 4 + 1 = 5 (left: 1 5 6) \n",
      "\n",
      "Current step for agent 1: 1 - 1 = 0 (left: 0 4 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 6 / 1 = 6 (left: 5 6) \n",
      "\n",
      "Current step for agent 1: 6 - 0 = 6 (left: 4 6) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: 5 - 6 = -1 (left: -1) \n",
      "\n",
      "Current step for agent 1: 4 + 6 = 10 (left: 10) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: None \n",
      "\n",
      "Current step for agent 1: 10 + 10 = 20 (left: 20) \n",
      "\n",
      "agent_id:  0\n",
      "agent_id:  1\n",
      "reflexions per agent {0: [\"Reflection:\\nThe mistake in the previous attempt is the operation 5 - 6 = -1, which results in a negative number and does not lead to a solution. This mistake can be avoided by considering the properties of the target number 24, which is positive. A general reflection is to prioritize operations that preserve or increase the value of the numbers, especially when the target is a positive number. Additionally, it's essential to consider the limitations and potential outcomes of each operation to avoid dead ends.\"], 1: ['The previous attempt contains a mistake in the last step, where it states \"10 + 10 = 20\" without a valid operation to obtain the second 10. This error occurs because the attempt tries to reuse a result (10) that is not available as an operand for the next operation.\\n\\nTo avoid similar mistakes, it\\'s essential to ensure that each step only uses the numbers and intermediate results that are actually available. A general reflection is that players should carefully track the numbers and results obtained at each step and only use valid and existing operands for subsequent operations.']}\n",
      "Step 0 : Stepping\n",
      "Current step for agent 0: 6 * 4 = 24 (left: 1 1 24) \n",
      "\n",
      "Current step for agent 1: 4 - 1 = 3 (left: 3 1 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 1 + 24 = 25 (left: 1 25) \n",
      "\n",
      "Current step for agent 1: 6 - 1 = 5 (left: 3 5) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: 1 - 25 = -24 (left: -24) \n",
      "\n",
      "Current step for agent 1: 5 / 3 = 1.67 (left: 1.67) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: No possible next steps, only one number remains. \n",
      "\n",
      "Current step for agent 1: No possible next steps, only one number is available. \n",
      "\n",
      "agent_id:  0\n",
      "agent_id:  1\n",
      "reflexions per agent {0: [\"Reflection:\\nThe mistake in the previous attempt is the operation 5 - 6 = -1, which results in a negative number and does not lead to a solution. This mistake can be avoided by considering the properties of the target number 24, which is positive. A general reflection is to prioritize operations that preserve or increase the value of the numbers, especially when the target is a positive number. Additionally, it's essential to consider the limitations and potential outcomes of each operation to avoid dead ends.\", \"The mistake in the previous attempt is that it did not achieve the goal of equating to 24, and it also resulted in only one number remaining, limiting further operations. This mistake can be avoided by ensuring that each step leaves multiple numbers or viable options for further operations. A general reflection is that in the game of 24, it's crucial to prioritize operations that maintain flexibility and multiple possible paths, rather than narrowing down to a single number too quickly.\"], 1: ['The previous attempt contains a mistake in the last step, where it states \"10 + 10 = 20\" without a valid operation to obtain the second 10. This error occurs because the attempt tries to reuse a result (10) that is not available as an operand for the next operation.\\n\\nTo avoid similar mistakes, it\\'s essential to ensure that each step only uses the numbers and intermediate results that are actually available. A general reflection is that players should carefully track the numbers and results obtained at each step and only use valid and existing operands for subsequent operations.', 'Reflection:\\nThe mistake in the previous attempt is the division operation (5 / 3) that resulted in a non-integer value (1.67), making it difficult to proceed with basic arithmetic operations to reach 24. This mistake can be avoided by prioritizing operations that maintain integer results, especially in the early stages of the puzzle. A general reflection is to favor operations that preserve integers and avoid creating non-integer values prematurely, as they can limit subsequent possibilities and make it harder to reach the target result of 24.']}\n",
      "Step 0 : Stepping\n",
      "Current step for agent 0: 6 * 1 = 6 (left: 1 4 6) \n",
      "\n",
      "Current step for agent 1: 6 + 4 = 10 (left: 1 10) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 1 + 6 = 7 (left: 4 7) \n",
      "\n",
      "Current step for agent 1: 1 * 10 = 10 (left: 10) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: 7 / 4 = 1.75 (left: 1.75) \n",
      "\n",
      "Current step for agent 1: 10 + 10 = 20 (left: 20) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: 1.75 * 1.75 = 3.0625 (left: 3.0625) \n",
      "\n",
      "Current step for agent 1: 20 + 0 = 20 (left: 20) \n",
      "\n",
      "agent_id:  0\n",
      "agent_id:  1\n",
      "reflexions per agent {0: [\"The mistake in the previous attempt is that it did not achieve the goal of equating to 24, and it also resulted in only one number remaining, limiting further operations. This mistake can be avoided by ensuring that each step leaves multiple numbers or viable options for further operations. A general reflection is that in the game of 24, it's crucial to prioritize operations that maintain flexibility and multiple possible paths, rather than narrowing down to a single number too quickly.\", \"The previous attempt contains a mistake in the operation sequence, as it results in a non-integer value (1.75) and fails to reach 24. This mistake can be avoided by prioritizing operations that maintain integer results and exploring alternative sequences. A general reflection is that in the game of 24, it's essential to favor operations that preserve integers and to systematically explore different combinations of numbers and operations to increase the chances of finding a valid solution.\"], 1: ['Reflection:\\nThe mistake in the previous attempt is the division operation (5 / 3) that resulted in a non-integer value (1.67), making it difficult to proceed with basic arithmetic operations to reach 24. This mistake can be avoided by prioritizing operations that maintain integer results, especially in the early stages of the puzzle. A general reflection is to favor operations that preserve integers and avoid creating non-integer values prematurely, as they can limit subsequent possibilities and make it harder to reach the target result of 24.', 'The previous attempt contains a mistake in the step \"20 + 0 = 20\", as adding 0 does not contribute to reaching the goal of 24 and is an unnecessary operation. This mistake can be avoided by ensuring that each operation brings the result closer to 24. A general reflection is that players should focus on making progress towards the target number with each operation, rather than performing redundant or non-contributing calculations.']}\n",
      "Step 0 : Stepping\n",
      "Current step for agent 0: 4 + 6 = 10 (left: 1 1 10) \n",
      "\n",
      "Current step for agent 1: 4 / 1 = 4 (left: 4 4 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 1 + 1 = 2 (left: 2 10) \n",
      "\n",
      "Current step for agent 1: 4 - 6 = -2 (left: -2 4) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: 10 / 2 = 5 (left: 5) \n",
      "\n",
      "Current step for agent 1: -2 - 4 = -6 (left: -6) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: None (left: 5) \n",
      "\n",
      "Current step for agent 1: There are no possible next steps with only one number. \n",
      "\n",
      "agent_id:  0\n",
      "agent_id:  1\n",
      "reflexions per agent {0: [\"The previous attempt contains a mistake in the operation sequence, as it results in a non-integer value (1.75) and fails to reach 24. This mistake can be avoided by prioritizing operations that maintain integer results and exploring alternative sequences. A general reflection is that in the game of 24, it's essential to favor operations that preserve integers and to systematically explore different combinations of numbers and operations to increase the chances of finding a valid solution.\", \"The previous attempt failed to reach 24, ending with 5. The mistake lies in the lack of exploration of alternative operations and combinations. Specifically, the attempt only considered addition and division, neglecting multiplication and subtraction. To avoid similar mistakes, it's essential to systematically explore all possible operations and combinations of the given numbers. A general reflection is that a thorough and systematic approach is crucial in solving math puzzles like the game of 24, where a single overlooked operation or combination can lead to an incorrect or incomplete solution.\"], 1: ['The previous attempt contains a mistake in the step \"20 + 0 = 20\", as adding 0 does not contribute to reaching the goal of 24 and is an unnecessary operation. This mistake can be avoided by ensuring that each operation brings the result closer to 24. A general reflection is that players should focus on making progress towards the target number with each operation, rather than performing redundant or non-contributing calculations.', 'Reflection:\\nThe mistake in the previous attempt is the premature subtraction operation (4 - 6 = -2) which led to a negative number and limited subsequent operations. This mistake can be avoided by prioritizing operations that combine numbers in a way that preserves possibilities for future calculations, such as multiplication or addition. A general reflection is to favor operations that increase the number of potential next steps, rather than narrowing them down too quickly.']}\n",
      "Step 0 : Stepping\n",
      "Current step for agent 0: 4 / 1 = 4 (left: 1 4 6) \n",
      "\n",
      "Current step for agent 1: 1 + 4 = 5 (left: 1 5 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 4 / 1 = 4 (left: 4 6) \n",
      "\n",
      "Current step for agent 1: 6 - 1 = 5 (left: 5 5) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: 4 + 6 = 10 (left: 10) \n",
      "\n",
      "Current step for agent 1: 5 + 5 = 10 (left: 10) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: 10 * 10 = 100 (left: 100) \n",
      "\n",
      "Current step for agent 1: 10 + 10 = 20 (left: 20) \n",
      "\n",
      "agent_id:  0\n",
      "agent_id:  1\n",
      "reflexions per agent {0: [\"The previous attempt failed to reach 24, ending with 5. The mistake lies in the lack of exploration of alternative operations and combinations. Specifically, the attempt only considered addition and division, neglecting multiplication and subtraction. To avoid similar mistakes, it's essential to systematically explore all possible operations and combinations of the given numbers. A general reflection is that a thorough and systematic approach is crucial in solving math puzzles like the game of 24, where a single overlooked operation or combination can lead to an incorrect or incomplete solution.\", 'The previous attempt contains a mistake. The error occurs when the operation 10 * 10 = 100 is performed, as this step is unnecessary and leads to a result far from the target of 24. This mistake can be avoided by focusing on using the remaining numbers to get closer to 24, rather than performing operations that drastically increase the result.\\n\\nA general reflection is that players should prioritize using operations that simplify the numbers or bring them closer to the target, rather than complicating the result. This can be achieved by carefully evaluating the remaining numbers and choosing operations that have a higher likelihood of yielding a result near 24.'], 1: ['Reflection:\\nThe mistake in the previous attempt is the premature subtraction operation (4 - 6 = -2) which led to a negative number and limited subsequent operations. This mistake can be avoided by prioritizing operations that combine numbers in a way that preserves possibilities for future calculations, such as multiplication or addition. A general reflection is to favor operations that increase the number of potential next steps, rather than narrowing them down too quickly.', 'The previous attempt contains a mistake in the step \"10 + 10 = 20\" because the number 10 is not available to be added to itself since it was the result of a previous operation and not one of the original numbers. This mistake can be avoided by keeping track of the numbers and intermediate results available at each step. A general reflection is that players should be mindful of the numbers they have at each step and ensure that they are using valid and available values to perform operations.']}\n",
      "Step 0 : Stepping\n",
      "Current step for agent 0: 1 + 6 = 7 (left: 1 4 7) \n",
      "\n",
      "Current step for agent 1: 1 - 1 = 0 (left: 0 4 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 4 + 7 = 11 (left: 1 11) \n",
      "\n",
      "Current step for agent 1: 0 + 6 = 6 (left: 4 6) \n",
      "\n",
      "Step 2 : Stepping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1612, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1659, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100132, Requested 326. Please try again in 6m36.329s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 5 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100132, Requested 326. Please try again in 6m36.329s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Current step for agent 0: 1 - 11 = -10 (left: -10) \n",
      "\n",
      "Current step for agent 1: 6 - 4 = 2 (left: 2) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: None (left: -10) \n",
      "\n",
      "Current step for agent 1: None (left: 2) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "await runReflexionGameOf24(\"k most recent\", 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : Stepping\n",
      "Current step for agent 0: 4 - 1 = 3 (left: 1 3 6) \n",
      "\n",
      "Current step for agent 1: 1 / 1 = 1 (left: 1 4 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 6 / 1 = 6 (left: 3 6) \n",
      "\n",
      "Current step for agent 1: 6 / 1 = 6 (left: 4 6) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Rate limit error, sleeping for 5 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100122, Requested 153. Please try again in 3m57.735s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit error, sleeping for 5 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100122, Requested 153. Please try again in 3m57.674s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100122, Requested 153. Please try again in 3m57.735s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100122, Requested 153. Please try again in 3m57.674s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step for agent 0: 3 - 6 = -3 (left: -3) \n",
      "\n",
      "Current step for agent 1: 6 / 4 = 1.5 (left: 1.5) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Rate limit error, sleeping for 5 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100115, Requested 153. Please try again in 3m52.016s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit error, sleeping for 5 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100115, Requested 153. Please try again in 3m51.934s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100115, Requested 153. Please try again in 3m52.016s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100115, Requested 153. Please try again in 3m51.934s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100109, Requested 153. Please try again in 3m46.928s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100109, Requested 153. Please try again in 3m46.87s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 15 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100109, Requested 153. Please try again in 3m46.928s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit error, sleeping for 45 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100109, Requested 153. Please try again in 3m46.87s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100092, Requested 153. Please try again in 3m31.841s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100092, Requested 153. Please try again in 3m31.841s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100057, Requested 153. Please try again in 3m1.733999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnehf4y7ekstchs86eqswyvy` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100057, Requested 153. Please try again in 3m1.733999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100863, Requested 153. Please try again in 14m38.088s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100863, Requested 153. Please try again in 14m38.088s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100828, Requested 153. Please try again in 14m7.983999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100828, Requested 153. Please try again in 14m7.983999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100793, Requested 153. Please try again in 13m37.922s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100793, Requested 153. Please try again in 13m37.922s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100758, Requested 153. Please try again in 13m7.845s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100758, Requested 153. Please try again in 13m7.845s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100724, Requested 153. Please try again in 12m37.783s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100724, Requested 153. Please try again in 12m37.783s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100689, Requested 153. Please try again in 12m7.705999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100689, Requested 153. Please try again in 12m7.705999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100654, Requested 153. Please try again in 11m37.665s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100654, Requested 153. Please try again in 11m37.665s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100619, Requested 153. Please try again in 11m7.604999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100619, Requested 153. Please try again in 11m7.604999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100584, Requested 153. Please try again in 10m37.569s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100584, Requested 153. Please try again in 10m37.569s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100550, Requested 153. Please try again in 10m7.468s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100550, Requested 153. Please try again in 10m7.468s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100515, Requested 153. Please try again in 9m37.446s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100515, Requested 153. Please try again in 9m37.446s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100480, Requested 153. Please try again in 9m7.309999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100480, Requested 153. Please try again in 9m7.309999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100445, Requested 153. Please try again in 8m37.306s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100445, Requested 153. Please try again in 8m37.306s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100410, Requested 153. Please try again in 8m7.075999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100410, Requested 153. Please try again in 8m7.075999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100376, Requested 153. Please try again in 7m37.157999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100376, Requested 153. Please try again in 7m37.157999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100341, Requested 153. Please try again in 7m6.96s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100341, Requested 153. Please try again in 7m6.96s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100306, Requested 153. Please try again in 6m37.04s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100306, Requested 153. Please try again in 6m37.04s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/FoA-1/async_engine/api.py\", line 276, in lazykey_request\n",
      "    single_response = await asyncio.wait_for(self.clients[model].request(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/lazykey/keyhandler.py\", line 45, in request\n",
      "    response = await key.client.chat.completions.create(*args, **kwargs)  # Awaiting async call\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 649, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1818, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1526, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jonas/anaconda3/envs/foa/lib/python3.12/site-packages/groq/_base_client.py\", line 1627, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100271, Requested 153. Please try again in 6m6.817999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit error, sleeping for 60 seconds\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jnef1hnrep9800d194n6k1mx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100271, Requested 153. Please try again in 6m6.817999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "await runReflexionGameOf24(\"summary\", 5, 5, summary_method=\"incremental\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : Stepping\n",
      "Current step for agent 0: 1 - 1 = 0 (left: 0 4 6) \n",
      "\n",
      "Current step for agent 1: 4 - 1 = 3 (left: 1 3 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 4 / 6 = 2/3 (left: 0 2/3) \n",
      "\n",
      "Current step for agent 1: 3 * 6 = 18 (left: 1 18) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: 0 + 2/3 = 2/3 (left: 2/3) \n",
      "\n",
      "Current step for agent 1: 1 + 18 = 19 (left: 19) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: 3 - 2 = 1 (left: 1) \n",
      "\n",
      "Current step for agent 1: None \n",
      "\n",
      "agent_id:  0\n",
      "agent_id:  1\n",
      "reflexions per agent {0: ['To solve the game of 24, ensure that all operations are performed using only the numbers provided in the input or resulting from previous calculations, carefully tracking the numbers available at each step to avoid introducing extraneous numbers.'], 1: ['To avoid mistakes in the game of 24, consider all possible combinations of operations and explore multiple branches, rather than limiting yourself to a linear sequence of calculations, and avoid getting fixated on a single approach.']}\n",
      "Step 0 : Stepping\n",
      "Current step for agent 0: 1 + 1 = 2 (left: 2 4 6) \n",
      "\n",
      "Current step for agent 1: 4 / 1 = 4 (left: 4 4 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 2 + 6 = 8 (left: 4 8) \n",
      "\n",
      "Current step for agent 1: 4 - 4 = 0 (left: 0 6) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: 4 * 8 = 32 (left: 32) \n",
      "\n",
      "Current step for agent 1: 0 * 6 = 0 (left: 0) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: No possible next steps, only one number is available. \n",
      "\n",
      "Current step for agent 1: None \n",
      "\n",
      "agent_id:  0\n",
      "agent_id:  1\n",
      "reflexions per agent {0: ['To solve the game of 24, two key reflections are important:\\n\\n1. Carefully track the numbers available at each step and only use valid numbers to perform operations, avoiding the introduction of extraneous numbers.\\n2. Prioritize operations that maintain flexibility, such as addition and subtraction, and avoid multiplication and division operations that can quickly escalate or reduce the values, making it harder to reach the target of 24.'], 1: ['To solve the game of 24, avoid mistakes by:\\n1. Exploring all possible combinations of operations, considering multiple branches and not limiting to a linear sequence.\\n2. Carefully evaluating intermediate results to avoid operations that lead to zeros or dead ends, prioritizing those that preserve flexibility and maximize the potential to reach 24.']}\n",
      "Step 0 : Stepping\n",
      "Current step for agent 0: 1 - 6 = -5 (left: -5 1 4) \n",
      "\n",
      "Current step for agent 1: 1 - 4 = -3 (left: -3 1 6) \n",
      "\n",
      "Step 1 : Stepping\n",
      "Current step for agent 0: 1 + 4 = 5 (left: -5 5) \n",
      "\n",
      "Current step for agent 1: -3 - 6 = -9 (left: -9 1) \n",
      "\n",
      "Step 2 : Stepping\n",
      "Current step for agent 0: -5 + 5 = 0 (left: 0) \n",
      "\n",
      "Current step for agent 1: (-9) - 1 = -10 (left: -10) \n",
      "\n",
      "Step 3 : Stepping\n",
      "Current step for agent 0: No possible next steps (left: 0) \n",
      "\n",
      "Current step for agent 1: None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "await runReflexionGameOf24(\"summary\", 2, 2, summary_method=\"all_previous\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
